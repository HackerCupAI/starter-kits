{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HackerCupAI/starter-kits/blob/main/rag/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{rag-hackercup} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "</a>\n",
    "\n",
    "\n",
    "In this notebook, we will build a few Code Generation agents for the [HackerCup AI](https://hackercupai.github.io/) challenge.\n",
    "\n",
    "We will build three different agents using different techniques and evaluate them using [W&B Weave](https://weave-docs.wandb.ai/).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/weave/master/docs/static/img/evals-hero.png\" width=\"800\" height=\"450\">\n",
    "\n",
    "A more detailed walkthough of the approach we will use in this notebook can be found in the following Youtube video:\n",
    "Hint: Click on the image to watch the video ðŸ˜Ž\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.youtube.com/watch?v=cObBj2UpWK8\">\n",
    "<img src=\"https://img.youtube.com/vi/cObBj2UpWK8/0.jpg\" width=\"600\" height=\"450\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weave\n",
    "\n",
    "\n",
    "Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights & Biases. We will use the following weave to trace and evaluate the various agents we build.\n",
    "\n",
    "We will use Weave to keep track and evaluate the different agents we build.\n",
    "\n",
    "Our goal is to bring rigor, best-practices, and composability to the inherently experimental process of developing AI applications, without introducing cognitive overhead.\n",
    "\n",
    "If you want to learn more about Weave, you can [get started](https://weave-docs.wandb.ai/quickstart) by decorating Python functions with `@weave.op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dataset\n",
    "\n",
    "We will use [HackerCup dataset](https://huggingface.co/datasets/hackercupai/hackercup) in this notebook.\n",
    "\n",
    "Specifically, the **practice** dataset from the **2023** season.\n",
    "\n",
    "We have already downloaded the dataset for you and saved it as a wandb Arfifact. \n",
    "You can either use the artifact by running the next cell or download the dataset using the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starter-kits repo\n",
    "!git clone https://github.com/HackerCupAI/starter-kits\n",
    "# Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "%cd starter-kits/rag\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again **you only need to run the following cell only once** to download the dataset and the retriever\n",
    "**You can comment out the cell after it's run once.**\n",
    "We will use the dataset to load some practice problems and solutions from the HackerCup dataset and evaluate our agents on it.\n",
    "We will use the retriver later in the notebook to build the RAG agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Uncomment and run this line if you haven't logged in to wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"codegen-starter\", job_type=\"download_dataset\")\n",
    "\n",
    "dataset_artifact = run.use_artifact(\n",
    "    \"parambharat/hackercup/practice_dataset:v0\", type=\"dataset\"\n",
    ")\n",
    "dataset_dir = dataset_artifact.download()\n",
    "dataset_dir = pathlib.Path(dataset_dir)\n",
    "\n",
    "retriever_artifact = run.use_artifact(\n",
    "    \"parambharat/hackercup/retriever:v0\", type=\"model\"\n",
    ")\n",
    "retriever_dir = retriever_artifact.download()\n",
    "retriever_dir = pathlib.Path(retriever_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can download the dataset by running the download script from the [submit-first-solution](https://github.com/HackerCupAI/starter-kits/tree/main/submit_first_solution). Specifically, you can run the following command to download the dataset:\n",
    "\n",
    "```bash\n",
    "python download.py --year 2023 --dataset_folder dataset\n",
    "```\n",
    "\n",
    "\n",
    "This should create a `dataset` folder with the problems and solutions.\n",
    "\n",
    "Here's an example of what the data looks like for the `dim_sum_delivery` problem from the `2023` season:\n",
    "\n",
    "```\n",
    "data/dataset/2023/practice\n",
    "...\n",
    "â”œâ”€â”€ dim_sum_delivery.cpp\n",
    "â”œâ”€â”€ dim_sum_delivery.in\n",
    "â”œâ”€â”€ dim_sum_delivery.md\n",
    "â”œâ”€â”€ dim_sum_delivery.out\n",
    "â”œâ”€â”€ dim_sum_delivery_sample_input.txt\n",
    "â”œâ”€â”€ dim_sum_delivery_sample_output.txt\n",
    "â”œâ”€â”€ dim_sum_delivery_sol.md\n",
    "...\n",
    "```\n",
    "\n",
    "Each problem has a `in`, `out`, `md`, `cpp`, and `sol` file.\n",
    "\n",
    "The `in` file contains the input data for the problem.\n",
    "The `out` file contains the expected output for the problem.\n",
    "The `md` file contains the problem statement.\n",
    "The `cpp` file contains the source code to the solution.\n",
    "The `sol` file contains the detailed solution to the problem.\n",
    "The `sample_input.txt` and `sample_output.txt` files contain the sample input and output for the problem. These are the test cases that will be available to the agent during development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "import weave\n",
    "from nest_asyncio import apply\n",
    "\n",
    "apply()\n",
    "\n",
    "# Some logging to see the progress\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weave Setup\n",
    "\n",
    "WEAVE_PROJECT = \"hackercup\"  # REPLACE WITH YOUR PROJECT NAME\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Agent\n",
    "\n",
    "For our first agent, we will use a `zero-shot solver`.\n",
    "It's a simple LLM API call with a detailed prompt to solve the problem.\n",
    "\n",
    "But first we need to load the problems and convert them to a more structured format and define a way to run the code and evaluate the solution.\n",
    "\n",
    "First we'll start with loading some utilities. While there are other utilities we load, the ones we care about the most are `load_problem` and `check_correctness`.\n",
    "\n",
    "The `load_problem` function will load a problem from our dataset into a more structured format.\n",
    "The `check_correctness` function will run the generated code and evaluate the solution against the expected output for the sample test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set the OpenAI API KEY for this session\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (FAST_LLM, STRONG_LLM, Problem, Solution, async_client,\n",
    "                   check_correctness, format_response, load_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a problem into a Pydantic model.\n",
    "problem = load_problem(\"dim_sum_delivery\", dataset_dir)\n",
    "print(problem.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check to see if the code evaluation works\n",
    "# We will use this to check the programs our the agents generate\n",
    "\n",
    "program_code = \"print('hello, world!')\"\n",
    "input_data = \"\"\n",
    "expected_output = \"hello, world!\"\n",
    "timeout = 2\n",
    "\n",
    "test_result = check_correctness(program_code, input_data, expected_output, timeout)\n",
    "print(\"Example 1: \", test_result)\n",
    "test_result = check_correctness(\"print('goodbye')\", input_data, \"hi there\", timeout)\n",
    "print(\"Example 2: \", test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to load a problem and evaluate a solution, let's define a prompt to solve the problem and create a simple agent to solve the problem. \n",
    "\n",
    "Here'e one such prompt we will use to solve the problem, it contains instructions for the model on how to solve the problem and the format of the response we expect from the model. Feel free to tweak the prompt if you like but this should work decently well for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVER_INSTRUCTIONS = \"\"\"You are a world-class competitive programmer tasked with solving a programming problem. \n",
    "You will be provided with a problem statement, and you need to create a Python3 solution for it. \n",
    "Your task it to develop a winning solution to the problem in Python3 programming language.\n",
    "You will do this in a step-by-step manner.\n",
    "\n",
    "Step 1: Extract the core question and the problem-solving information from the problem statement.\n",
    "Step 2: Describe the algorithm used to solve the problem.\n",
    "Step 3: Write a short tutorial on the algorithm and how it works.\n",
    "Step 4: Generate a step by step plan to solve the problem.\n",
    "Step 5: Generate the pseudocode to solve the problem.\n",
    "Step 6: Write the final solution in Python3 programming language to solve the problem.\n",
    "\n",
    "Competition Guidelines:\n",
    "    a. Do not use any external libraries; stick to Python 3 standard library\n",
    "    b. Handle input and output using standard input/output (stdin/stdout)\n",
    "    c. Use helper functions to improve readability of the code.\n",
    "    c. Use the `input()` function to take input from stdin and print the output to stdout.\n",
    "    d. Do not add extra print statements otherwise it will fail the test cases.\n",
    "    e. Make sure your code passes all potential test cases, including edge cases\n",
    "    f. Follow the input/output format specified in the problem statement and the sample test cases.\n",
    "\n",
    "\n",
    "**Formatting Instructions: Your response must follow the following xml format** -\n",
    "\n",
    "<root>\n",
    "<core_question>\n",
    "[Extract core question, only the most comprehensive and detailed one!]\n",
    "</core_question>\n",
    "<problem_solving_info>\n",
    "[Extract problem-solving information related to the core question, only the most comprehensive and detailed one!]\n",
    "</problem_solving_info>\n",
    "<algorithm>\n",
    "[Algorithm to solve the problem. Describe the algorithm used to solve the problem such that a novice programmer without any prior knowledge of the solution can implement it. Do not generate code.]\n",
    "</algorithm>\n",
    "<tutorial>\n",
    "[Write a useful tutorial about the above mentioned algorithm(s). Provide a high level generic tutorial for solving these types of problems. Do not generate code.]\n",
    "</tutorial>\n",
    "<plan>\n",
    "[Generate a step by step plan to solve the problem.]\n",
    "</plan>\n",
    "<pseudocode>\n",
    "[Generate a pseudocode to solve the problem.]\n",
    "</pseudocode>\n",
    "<source_code>\n",
    "[Write the final solution in Python3 programming language to solve the problem.]\n",
    "</source_code>\n",
    "</root>\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Here we have defined a `Solution` model to enforce the format of the response we expect from the model.\n",
    "If you change the `SOLVER_INSTRUCTIONS`, you need to change the `Solution` model to enforce the new format.\n",
    "We use `format_response` to enforce the format of the response we expect from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def draft_solution(\n",
    "        problem: Problem, model: str = FAST_LLM, temperature: float = 0.0\n",
    ") -> Solution:\n",
    "    user_prompt = f\"\"\"{problem.as_xml}\n",
    "---\n",
    "Let's think step by step to solve the problem:\n",
    "\"\"\"\n",
    "\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SOLVER_INSTRUCTIONS},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_model=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    formatted_response = await format_response(\n",
    "        response.choices[0].message.content, Solution\n",
    "    )\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the main solution drafter ready, we can define the `zero_shot_solver` agent.\n",
    "The agent will use the `draft_solution` function to draft a solution and the `check_correctness` function to check the correctness of the generated solution and return the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def zero_shot_solver(\n",
    "        problem: Problem, model: str = FAST_LLM, temperature: float = 0.0, timeout: int = 10\n",
    ") -> dict:\n",
    "    logger.info(\"Drafting intial zero-shot solution\")\n",
    "    solution = await draft_solution(\n",
    "        problem=problem,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    test_report = check_correctness(\n",
    "        solution.source_code, problem.sample_input, problem.sample_output, timeout\n",
    "    )\n",
    "    logger.info(f\"Draft solution result: {repr(test_report)}\")\n",
    "    return {\"solution\": solution, \"test_report\": test_report, \"stage\": \"zero-shot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the zero-shot agent on the sample problem\n",
    "zero_shot_result = await zero_shot_solver(problem)\n",
    "print(\"*\" * 80)\n",
    "print(zero_shot_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(zero_shot_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple evaluation using weave to evaluate the zero-shot agent.\n",
    "You'll quickly see how this simple evaluation framework can become very powerful and will scale to very complex workflows.\n",
    "Our agent already takes care of running the code, evaluating the solution against the expected output for the sample test cases and returning the report in the model output.\n",
    "We expect that the `test_report` is `\"passed\"` in the agent output so we can use that to evaluate the agent. \n",
    "\n",
    "But first we need to load all the problems and convert them to a more structured format. A good agent should be able to handle all the problems in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_names = map(lambda x: x.stem, dataset_dir.rglob(\"*in\"))\n",
    "problems = list(map(lambda x: load_problem(x, dataset_dir), problem_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple depection of the evaluation.\n",
    "# We expect the output to be `\"passed\"` for all the problems if the agent is working correctly.\n",
    "examples = [{\"problem\": problem, \"expected\": \"passed\"} for problem in problems]\n",
    "\n",
    "\n",
    "# A simple scorer that checks if the code generated by agent passed the test case\n",
    "@weave.op\n",
    "def scorer(expected: str, model_output: dict) -> dict:\n",
    "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
    "\n",
    "\n",
    "# This is a simple evaluation that checks if the code generated by agent passed the test\n",
    "eval = weave.Evaluation(dataset=examples, scorers=[scorer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the zero-shot agent.\n",
    "We will create a `weave.Model` instance for the zero-shot agent.\n",
    "This will help us conduct robust experiments and comparisons by helping us track various settings and parameters for the agent.\n",
    "For now, we will focus on the `LLM` and the `temperature` for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing fancy here, just a model that takes in a problem and returns a solution\n",
    "\n",
    "\n",
    "class ZeroshotAgent(weave.Model):\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await zero_shot_solver(\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the zero shot agent for all the models and temperatures\n",
    "\n",
    "\n",
    "eval_models = [FAST_LLM, STRONG_LLM]\n",
    "eval_temperatures = [0.0, 0.5, 1.0]\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        zeroshot_agent = ZeroshotAgent(model=LLM, temperature=temperature, timeout=30)\n",
    "        zeroshot_results = eval.evaluate(zeroshot_agent)\n",
    "        tasks.append(zeroshot_results)\n",
    "\n",
    "# Phew that's 2(models)*3(temps)*5(problems) = 30 evaluations\n",
    "\n",
    "zeroshot_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the results you should also be able to visit your weave dashboard to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Agent\n",
    "\n",
    "The RAG agent is a more complex agent that uses the retriever to retrieve the similar problems and solutions, and then uses these as few-shot examples to a model to generate a new solution. We will be using the [codecontests](https://huggingface.co/datasets/deepmind/code_contests) dataset to find the similar problems and the solutions. \n",
    "\n",
    "Retriving similar problems and solutions for a given problem statement is a non-trivial task. It involves indexing a large corpus of problems and solutions and then using a search algorithm to find the most similar problems and solutions. We will use the `bm25` algorithm to index the problems and solutions. However, it's important to note that two problems with similar wording - Such as `Alice` and `Bob` are not similar problems. A keyword search algorithm like BM25 might not be able to find similar problems and solutions based on the problem statement due to this limitation. \n",
    "\n",
    "While we could use `semantic search` it, would require a lot of data and compute. Therefore, we will use the `bm25` algorithm to index the problems and solutions and then use our zero-shot agent to generate a solution for a given problem statement. Then we can look for similar problems and solutions using the generated solution by comparing the AST (Abstract Syntax Tree) of the problems and solutions. This is a very simplistic approach and is not perfect by any means, but it's a good starting point.\n",
    "\n",
    "\n",
    "For now, you can just load the retriever from the wandb artifact store below, however, If you wish to use your own data, you might need to pre-process the data and create the retriever. You can checkout `starter-kits/rag/retriever.py` for more details.\n",
    "\n",
    "\n",
    "However, simply using BM25 is not enough to find similar problems and solutions because two problems with similar solutions might have different problem statements and vice versa.\n",
    "\n",
    "Can use semantic search to mitigate this by finding the most similar problems and solutions from an initial candidate pool retrieved using BM25. This should keep our compute requirements in check. We can use the `cosine similarity` to find the most similar problems and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import describe_examples, format_examples\n",
    "from retriever import Retriever, rerank_docs\n",
    "\n",
    "logger.info(\"Loading retriever ... this may take a while ...\")\n",
    "retriever = Retriever.load(retriever_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build the RAG agent.\n",
    "\n",
    "As we laid out earlier, a RAG agent is a model that takes in a problem and returns a solution using the retriever to retrieve the similar problems and the solutions and then use the model to generate a new solution. We will use the `draft_solution` function to draft a solution for a given problem statement. Then we can look for similar problems and solutions using the generated solution by comparing the AST (Abstract Syntax Tree) of the solution to the solutions in our dataset. We will than present these are few-shot examples to the model to generate a new solution for the given problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def generate_solution(\n",
    "        problem: Problem, examples: str, model: str = STRONG_LLM, temperature: float = 0.0\n",
    ") -> Solution:\n",
    "    instructions_prompt = f\"\"\"{SOLVER_INSTRUCTIONS}\n",
    "\n",
    "You have previously solved the following problems in this competition:\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"{problem.as_xml}\n",
    "---\n",
    "Let's think step by step to solve the problem:\n",
    "\n",
    "\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    formatted_response = await format_response(\n",
    "        response.choices[0].message.content, Solution\n",
    "    )\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def rag_solver(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.0,\n",
    "        timeout: int = 10,\n",
    ") -> dict:\n",
    "    \"\"\"The RAG Solver\"\"\"\n",
    "\n",
    "    zero_shot_result = await zero_shot_solver(\n",
    "        problem=problem,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    solution = zero_shot_result[\"solution\"]\n",
    "    test_report = zero_shot_result[\"test_report\"]\n",
    "    if test_report == \"passed\":\n",
    "        return zero_shot_result\n",
    "    logger.info(\"Iterating on a RAG solution\")\n",
    "\n",
    "    @weave.op\n",
    "    async def create_examplars(\n",
    "            problem: Problem, solution: Solution, top_k: int = 50, top_n: int = 5\n",
    "    ):\n",
    "        logger.info(f\"Generating examplars:\")\n",
    "        retrieve_docs = retriever.retrieve(solution.source_code, top_k)\n",
    "        reranked_docs = await rerank_docs(problem, solution, retrieve_docs, top_n)\n",
    "        analyses = await describe_examples(reranked_docs)\n",
    "        examplars = format_examples(reranked_docs, analyses)\n",
    "        return examplars\n",
    "\n",
    "    @weave.op\n",
    "    async def rag_solution(\n",
    "            problem: Problem,\n",
    "            draft_solution: Solution,\n",
    "            model: str = STRONG_LLM,\n",
    "            temperature: float = 0.0,\n",
    "            timeout: int = timeout,\n",
    "    ) -> dict:\n",
    "        logger.info(f\"Generating RAG solution:\")\n",
    "        examplars = await create_examplars(problem, draft_solution)\n",
    "        rag_solution = await generate_solution(\n",
    "            problem=problem,\n",
    "            examples=examplars,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        test_report = check_correctness(\n",
    "            rag_solution.source_code,\n",
    "            problem.sample_input,\n",
    "            problem.sample_output,\n",
    "            timeout,\n",
    "        )\n",
    "        logger.info(f\"RAG Solution Result: {repr(test_report)}\")\n",
    "        return {\"solution\": rag_solution, \"test_report\": test_report}\n",
    "\n",
    "    rag_result = await rag_solution(problem, solution, model, temperature, timeout)\n",
    "    solution = rag_result[\"solution\"]\n",
    "    test_report = rag_result[\"test_report\"]\n",
    "    return {\"solution\": solution, \"stage\": \"rag\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result = await rag_solver(retriever, problem, timeout=30)\n",
    "print(\"*\" * 80)\n",
    "print(rag_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(rag_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we are now ready to evaluate the RAG agent.\n",
    "We will create a `weave.Model` instance for the RAG agent and evaluate it using the same evaluation framework we used for the zero-shot agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver(\n",
    "            retriever=self.retriever,\n",
    "            problem=Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG agent for all the models and temperatures\n",
    "\n",
    "\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_agent = RAGAgent(\n",
    "            retriever=retriever, model=LLM, temperature=temperature, timeout=30\n",
    "        )\n",
    "        rag_results = eval.evaluate(rag_agent)\n",
    "        tasks.append(rag_results)\n",
    "\n",
    "# Again, 30 evals for the RAG agent with different models and temperatures\n",
    "\n",
    "rag_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While the RAG agent is an improvement over the zero-shot agent, it's still not perfect.\n",
    "It's still susceptible to hallucinations and incorrect solutions. \n",
    "One way to mitigate this is to use reflection.\n",
    "We can use another LLM call to reflect on the solution and test results and improve it.\n",
    "We can then use the improved solution to generate new few-shot examples and repeat the process in a loop until we converge to a solution or the iteration limit is reached.\n",
    "\n",
    "Again, this is not the best approach to solve the problem and has a lot of room for improvement, but it should help us get towards a working solution.\n",
    "\n",
    "Here are the reflection instructions we will provide to the LLM to reflect on the solution and test results, feel free to change the instructions to improve the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_INSTRUCTIONS = \"\"\"You are a world-class competitive programmer with a keen eye for detail and problem solving. \n",
    "Your expertise is in algorithms and data structures. \n",
    "You have incorrectly answered the following programming problem. \n",
    "Your task is to reflect on the problem, your solution, and the correct answer.\n",
    "You will then use this information help you answer the same question in the future. \n",
    "First, explain why you answered the question incorrectly.\n",
    "Second, list the keywords that describe the type of your errors from most general to most specific.\n",
    "Third, solve the problem again, step-by-step, based on your knowledge of the correct answer.\n",
    "Fourth, create a list of detailed instructions to help you correctly solve this problem in the future.\n",
    "Finally, create a list of general advice to help you solve similar types of problems in the future.\n",
    "Be concise in your response; however, capture all of the essential information.\n",
    "\n",
    "{problem}\n",
    "<incorrect_solution>\n",
    "{incorrect_solution}\n",
    "</incorrect_solution>\n",
    "<test_report>\n",
    "{test_report}\n",
    "</test_report>\n",
    "\n",
    "**Format Instructions: Your response must follow the following xml format** -\n",
    "\n",
    "<root>\n",
    "<reflection>\n",
    "[Reflect on the problem, your solution, and the correct answer.]\n",
    "</reflection>\n",
    "<keywords>\n",
    "[List the keywords that describe the type of your errors from most general to most specific.]\n",
    "</keywords>\n",
    "<step_by_step_solution>\n",
    "[Solve the problem again, step-by-step, based on your knowledge of the correct answer.]\n",
    "</step_by_step_solution>\n",
    "<instructions>\n",
    "[Create a list of detailed instructions to help you correctly solve this problem in the future.]\n",
    "</instructions>\n",
    "<general_advice>\n",
    "[Create a list of general advice to help you solve similar types of problems in the future.]\n",
    "</general_advice>\n",
    "</root>\n",
    "---\n",
    "Let's think step by step to reflect on the problem:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def reflection(\n",
    "        problem: Problem,\n",
    "        incorrect_solution: Solution,\n",
    "        test_report: str,\n",
    "        model: str = STRONG_LLM,\n",
    "        temperature: float = 0.0,\n",
    ") -> Reflection:\n",
    "    system_prompt = REFLECTION_INSTRUCTIONS.format(\n",
    "        problem=problem.as_xml,\n",
    "        incorrect_solution=incorrect_solution.as_xml,\n",
    "        test_report=test_report,\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    formatted_response = await format_response(\n",
    "        response.choices[0].message.content, Reflection\n",
    "    )\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def improve_solution(\n",
    "        problem: Problem,\n",
    "        incorrect_solution: Solution,\n",
    "        test_report: str,\n",
    "        reflections: Reflection,\n",
    "        model: str = STRONG_LLM,\n",
    "        temperature: float = 0.0,\n",
    ") -> Solution:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SOLVER_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": problem.as_xml},\n",
    "        {\"role\": \"assistant\", \"content\": incorrect_solution.as_xml},\n",
    "        {\"role\": \"user\", \"content\": f\"<test_report>\\n{test_report}\\n</test_report>\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Your previous answer to the question is incorrect. Please reflect on the problem, your solution, and the correct answer.\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": reflections.as_xml},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Use your self-reflection (above) to help you answer the question correctly.\n",
    "\n",
    "---\n",
    "Let's think step by step to solve the problem correctly:\n",
    "\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    formatted_response = await format_response(\n",
    "        response.choices[0].message.content, Solution\n",
    "    )\n",
    "    return formatted_response\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def rework_solution(\n",
    "        problem: Problem,\n",
    "        incorrect_solution: Solution,\n",
    "        test_report: str,\n",
    "        model: str = STRONG_LLM,\n",
    "        temperature: float = 0.0,\n",
    "        timeout: int = 10,\n",
    ") -> dict:\n",
    "    logger.info(f\"Reflecting and improving solution\")\n",
    "    reflections = await reflection(\n",
    "        problem=problem,\n",
    "        incorrect_solution=incorrect_solution,\n",
    "        test_report=test_report,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    improved_solution = await improve_solution(\n",
    "        problem=problem,\n",
    "        incorrect_solution=incorrect_solution,\n",
    "        test_report=test_report,\n",
    "        reflections=reflections,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    test_report = check_correctness(\n",
    "        improved_solution.source_code,\n",
    "        problem.sample_input,\n",
    "        problem.sample_output,\n",
    "        timeout,\n",
    "    )\n",
    "    logger.info(f\"Reworked solution result: {repr(test_report)}\")\n",
    "    return {\"solution\": improved_solution, \"test_report\": test_report}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def rag_solver_with_reflection(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.0,\n",
    "        max_iterations: int = 2,\n",
    "        timeout: int = 10,\n",
    "):\n",
    "    num_iterations = 0\n",
    "    test_report = \"failed\"\n",
    "    solution = None\n",
    "    while not test_report == \"passed\" and num_iterations < max_iterations:\n",
    "        rag_result = await rag_solver(\n",
    "            retriever=retriever,\n",
    "            problem=problem,\n",
    "            timeout=timeout,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        solution = rag_result[\"solution\"]\n",
    "        test_report = rag_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return rag_result\n",
    "        rework_result = await rework_solution(\n",
    "            problem=problem,\n",
    "            incorrect_solution=solution,\n",
    "            test_report=test_report,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        solution = rework_result[\"solution\"]\n",
    "        test_report = rework_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return {\n",
    "                \"solution\": solution,\n",
    "                \"stage\": \"reflection\",\n",
    "                \"test_report\": test_report,\n",
    "            }\n",
    "        num_iterations += 1\n",
    "    logger.info(\"Failed to generate a solution\")\n",
    "    return {\"solution\": solution, \"stage\": \"failed\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_result = await rag_solver_with_reflection(\n",
    "    retriever, problem, max_iterations=2, timeout=30\n",
    ")\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now, we are ready to evaluate a more complex agent that uses reflection\n",
    "This agent will try to solve the problem using the retriever\n",
    "and if it fails, it will ask the model to reflect on the problem\n",
    "and then re-work the solution\n",
    "and repeat this process for a fixed number of iterations\n",
    "or until the solution is correct or the iteration limit is reached\n",
    "\n",
    "But the best part is that we can use the same evaluation framework we used for the zero-shot and RAG agent to evaluate the RAG reflection agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG reflection agent for all the models and temperatures\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_reflection_agent = RAGReflectionAgent(\n",
    "            retriever=retriever, model=LLM, temperature=temperature, timeout=30\n",
    "        )\n",
    "        rag_reflection_results = eval.evaluate(rag_reflection_agent)\n",
    "        tasks.append(rag_reflection_results)\n",
    "rag_reflection_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that completes the demo!\n",
    "\n",
    "Key takeaways from this demo:\n",
    "1. We tried to solve some challenging competitive programming problems using LLM agents.\n",
    "2. We tried three different agents:\n",
    "    - Zero-shot agent\n",
    "    - RAG agent\n",
    "    - RAG reflection agent\n",
    "3. We used Weave to evaluate the agents and compare their performance.\n",
    "\n",
    "We hope you found this demo useful and interesting and that it gave you some ideas on how to use LLM agents to solve challenging problems.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
