{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HackerCupAI/starter-kits/blob/main/rag/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{rag-hackercup} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "</a>\n",
    "\n",
    "\n",
    "In this notebook, we will build a few Code Generation agents for the [HackerCup AI](https://hackercupai.github.io/) challenge.\n",
    "\n",
    "We will build three different agents using different techniques and evaluate them using [W&B Weave](https://weave-docs.wandb.ai/).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/weave/master/docs/static/img/evals-hero.png\" width=\"800\" height=\"450\">\n",
    "\n",
    "A more detailed walkthough of the approach we will use in this notebook can be found in the following Youtube video:\n",
    "Hint: Click on the image to watch the video ðŸ˜Ž\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.youtube.com/watch?v=cObBj2UpWK8\">\n",
    "<img src=\"https://img.youtube.com/vi/cObBj2UpWK8/0.jpg\" width=\"600\" height=\"450\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weave\n",
    "\n",
    "\n",
    "Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights & Biases. We will use the following weave to trace and evaluate the various agents we build.\n",
    "\n",
    "We will use Weave to keep track and evaluate the different agents we build.\n",
    "\n",
    "Our goal is to bring rigor, best-practices, and composability to the inherently experimental process of developing AI applications, without introducing cognitive overhead.\n",
    "\n",
    "If you want to learn more about Weave, you can [get started](https://weave-docs.wandb.ai/quickstart) by decorating Python functions with `@weave.op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starter-kits repo\n",
    "!git clone https://github.com/HackerCupAI/starter-kits\n",
    "# Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "%cd starter-kits/rag\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "# Weave Setup\n",
    "\n",
    "WEAVE_PROJECT = \"hackercup\"  # REPLACE WITH YOUR PROJECT NAME\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use [HackerCup dataset](https://huggingface.co/datasets/hackercupai/hackercup) in this notebook.\n",
    "\n",
    "Specifically, the **practice** dataset from the **2023** season.\n",
    "\n",
    "We have already processed the dataset and saved it as a [`weave.Dataset`](https://weave-docs.wandb.ai/guides/core-types/datasets/). You can either use the Dataset by running the next cell or download the dataset using the instructions below.\n",
    "\n",
    "We will use the dataset to load some practice problems and solutions from the HackerCup dataset and evaluate our agents on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set the OpenAI API KEY for this session\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    FAST_LLM,\n",
    "    STRONG_LLM,\n",
    "    Problem,\n",
    "    Solution,\n",
    "    async_client,\n",
    "    check_correctness,\n",
    "    format_response,\n",
    ")\n",
    "\n",
    "practice_dataset_uri = \"weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w\"\n",
    "problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]\n",
    "problems = list(map(lambda x: Problem(**x), problems_dataset))\n",
    "problem = problems[0]\n",
    "print(\"Sample Problem:\\n\\n\", problem.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can download the dataset by running the download script from the [submit-first-solution](https://github.com/HackerCupAI/starter-kits/tree/main/submit_first_solution). Specifically, you can run the following command to download the dataset:\n",
    "\n",
    "```bash\n",
    "python download.py --year 2023 --dataset_folder data\n",
    "```\n",
    "\n",
    "\n",
    "This should create a `dataset` folder with the problems and solutions.\n",
    "\n",
    "Here's an example of what the data looks like for the `dim_sum_delivery` problem from the `2023` season:\n",
    "\n",
    "```\n",
    "data/dataset/2023/practice\n",
    "...\n",
    "â”œâ”€â”€ dim_sum_delivery.cpp\n",
    "â”œâ”€â”€ dim_sum_delivery.in\n",
    "â”œâ”€â”€ dim_sum_delivery.md\n",
    "â”œâ”€â”€ dim_sum_delivery.out\n",
    "â”œâ”€â”€ dim_sum_delivery_sample_input.txt\n",
    "â”œâ”€â”€ dim_sum_delivery_sample_output.txt\n",
    "â”œâ”€â”€ dim_sum_delivery_sol.md\n",
    "...\n",
    "```\n",
    "\n",
    "Each problem has a `in`, `out`, `md`, `cpp`, and `sol` file.\n",
    "\n",
    "The `in` file contains the input data for the problem.\n",
    "The `out` file contains the expected output for the problem.\n",
    "The `md` file contains the problem statement.\n",
    "The `cpp` file contains the source code to the solution.\n",
    "The `sol` file contains the detailed solution to the problem.\n",
    "The `sample_input.txt` and `sample_output.txt` files contain the sample input and output for the problem. These are the test cases that will be available to the agent during development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from nest_asyncio import apply\n",
    "\n",
    "apply()\n",
    "\n",
    "# Some logging to see the progress\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Agent\n",
    "\n",
    "For our first agent, we will use a `zero-shot solver`.\n",
    "It's a simple LLM API call with a detailed prompt to solve the problem.\n",
    "\n",
    "But first we need to load the problems and convert them to a more structured format and define a way to run the code and evaluate the solution.\n",
    "\n",
    "First we'll start with loading some utilities. While there are other utilities we load, the ones we care about the most are `load_problem` and `check_correctness`.\n",
    "\n",
    "The `load_problem` function will load a problem from our dataset into a more structured format.\n",
    "The `check_correctness` function will run the generated code and evaluate the solution against the expected output for the sample test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check to see if the code evaluation works\n",
    "# We will use this to check the programs our the agents generate\n",
    "\n",
    "program_code = \"print('hello, world!')\"\n",
    "input_data = \"\"\n",
    "expected_output = \"hello, world!\"\n",
    "timeout = 2\n",
    "\n",
    "test_result = check_correctness(program_code, input_data, expected_output, timeout)\n",
    "print(\"Example 1: \", test_result)\n",
    "test_result = check_correctness(\"print('goodbye')\", input_data, \"hi there\", timeout)\n",
    "print(\"Example 2: \", test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to load a problem and evaluate a solution, let's define a prompt to solve the problem and create a simple agent to solve the problem. \n",
    "\n",
    "Here'e one such prompt we will use to solve the problem, it contains instructions for the model on how to solve the problem and the format of the response we expect from the model. Feel free to tweak the prompt if you like but this should work decently well for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import SOLVER_INSTRUCTIONS\n",
    "\n",
    "print(SOLVER_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Here we have defined a `Solution` model to enforce the format of the response we expect from the model.\n",
    "If you change the `SOLVER_INSTRUCTIONS`, you need to change the `Solution` model to enforce the new format.\n",
    "We use `format_response` to enforce the format of the response we expect from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def draft_solution(\n",
    "        problem: Problem, model: str = FAST_LLM, temperature: float = 0.0\n",
    ") -> Solution:\n",
    "    user_prompt = f\"\"\"{problem.as_xml}\n",
    "---\n",
    "Let's think step by step to solve the problem:\n",
    "\"\"\"\n",
    "\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SOLVER_INSTRUCTIONS},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_model=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    formatted_response = await format_response(\n",
    "        response.choices[0].message.content, Solution\n",
    "    )\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the main solution drafter ready, we can define the `zero_shot_solver` agent.\n",
    "The agent will use the `draft_solution` function to draft a solution and the `check_correctness` function to check the correctness of the generated solution and return the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def zero_shot_solver(\n",
    "        problem: Problem, model: str = FAST_LLM, temperature: float = 0.0, timeout: int = 10\n",
    ") -> dict:\n",
    "    logger.info(\"Drafting intial zero-shot solution\")\n",
    "    solution = await draft_solution(\n",
    "        problem=problem,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    test_report = check_correctness(\n",
    "        solution.source_code, problem.sample_input, problem.sample_output, timeout\n",
    "    )\n",
    "    logger.info(f\"Draft solution result: {repr(test_report)}\")\n",
    "    return {\"solution\": solution, \"test_report\": test_report, \"stage\": \"zero-shot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the zero-shot agent on the sample problem\n",
    "zero_shot_result = await zero_shot_solver(problem)\n",
    "print(\"*\" * 80)\n",
    "print(zero_shot_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(zero_shot_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple evaluation using weave to evaluate the zero-shot agent.\n",
    "You'll quickly see how this simple evaluation framework can become very powerful and will scale to very complex workflows.\n",
    "Our agent already takes care of running the code, evaluating the solution against the expected output for the sample test cases and returning the report in the model output.\n",
    "We expect that the `test_report` is `\"passed\"` in the agent output so we can use that to evaluate the agent. \n",
    "\n",
    "But first we need to load all the problems and convert them to a more structured format. A good agent should be able to handle all the problems in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple depection of the evaluation.\n",
    "# We expect the output to be `\"passed\"` for all the problems if the agent is working correctly.\n",
    "examples = [{\"problem\": problem, \"expected\": \"passed\"} for problem in problems]\n",
    "\n",
    "\n",
    "# A simple scorer that checks if the code generated by agent passed the test case\n",
    "@weave.op\n",
    "def scorer(expected: str, model_output: dict) -> dict:\n",
    "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
    "\n",
    "\n",
    "# This is a simple evaluation that checks if the code generated by agent passed the test\n",
    "eval = weave.Evaluation(dataset=examples, scorers=[scorer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the zero-shot agent.\n",
    "We will create a `weave.Model` instance for the zero-shot agent.\n",
    "This will help us conduct robust experiments and comparisons by helping us track various settings and parameters for the agent.\n",
    "For now, we will focus on the `LLM` and the `temperature` for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing fancy here, just a model that takes in a problem and returns a solution\n",
    "\n",
    "\n",
    "class ZeroshotAgent(weave.Model):\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await zero_shot_solver(\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the zero shot agent for all the models and temperatures\n",
    "eval_models = [FAST_LLM, STRONG_LLM]\n",
    "eval_temperatures = [0.0, 0.5, 1.0]\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        zeroshot_agent = ZeroshotAgent(model=LLM, temperature=temperature, timeout=30)\n",
    "        zeroshot_results = eval.evaluate(zeroshot_agent)\n",
    "        tasks.append(zeroshot_results)\n",
    "\n",
    "# Phew that's 2(models)*3(temps)*5(problems) = 30 evaluations\n",
    "\n",
    "zeroshot_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the results you should also be able to visit your weave dashboard to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Agent\n",
    "\n",
    "The RAG agent is a more complex agent that uses the retriever to retrieve the similar problems and solutions, and then uses these as few-shot examples to a model to generate a new solution. We will be using the [codecontests](https://huggingface.co/datasets/deepmind/code_contests) dataset to find the similar problems and the solutions. \n",
    "\n",
    "Retriving similar problems and solutions for a given problem statement is a non-trivial task. It involves indexing a large corpus of problems and solutions and then using a search algorithm to find the most similar problems and solutions. We will use the `bm25` algorithm to index the problems and solutions. However, it's important to note that two problems with similar wording - Such as `Alice` and `Bob` are not similar problems. A keyword search algorithm like BM25 might not be able to find similar problems and solutions based on the problem statement due to this limitation. \n",
    "\n",
    "While we could use `semantic search` it, would require a lot of data and compute. Therefore, we will use the `bm25` algorithm to index the problems and solutions and then use our zero-shot agent to generate a solution for a given problem statement. Then we can look for similar problems and solutions using the generated solution by comparing the AST (Abstract Syntax Tree) of the problems and solutions. This is a very simplistic approach and is not perfect by any means, but it's a good starting point.\n",
    "\n",
    "\n",
    "For now, you can just load the retriever below, however, If you wish to use your own data, you might need to pre-process the data and create the retriever. You can checkout `starter-kits/rag/retriever.py` for more details.\n",
    "\n",
    "\n",
    "However, simply using BM25 is not enough to find similar problems and solutions because two problems with similar solutions might have different problem statements and vice versa.\n",
    "\n",
    "Can use semantic search to mitigate this by finding the most similar problems and solutions from an initial candidate pool retrieved using BM25. This should keep our compute requirements in check. We can use the `cosine similarity` to find the most similar problems and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import describe_examples, format_examples, generate_solution\n",
    "from retriever import Retriever, rerank_docs\n",
    "\n",
    "logger.info(\"Loading retriever ... this may take a while ...\")\n",
    "retriever = Retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build the RAG agent.\n",
    "\n",
    "As we laid out earlier, a RAG agent is a model that takes in a problem and returns a solution using the retriever to retrieve the similar problems and the solutions and then use the model to generate a new solution. We will use the `draft_solution` function to draft a solution for a given problem statement. Then we can look for similar problems and solutions using the generated solution by comparing the AST (Abstract Syntax Tree) of the solution to the solutions in our dataset. We will than present these are few-shot examples to the model to generate a new solution for the given problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def rag_solver(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.0,\n",
    "        timeout: int = 10,\n",
    ") -> dict:\n",
    "    \"\"\"The RAG Solver\"\"\"\n",
    "\n",
    "    zero_shot_result = await zero_shot_solver(\n",
    "        problem=problem,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    solution = zero_shot_result[\"solution\"]\n",
    "    test_report = zero_shot_result[\"test_report\"]\n",
    "    if test_report == \"passed\":\n",
    "        return zero_shot_result\n",
    "    logger.info(\"Iterating on a RAG solution\")\n",
    "\n",
    "    @weave.op\n",
    "    async def create_examplars(\n",
    "            problem: Problem, solution: Solution, top_k: int = 50, top_n: int = 5\n",
    "    ):\n",
    "        logger.info(f\"Generating examplars:\")\n",
    "        retrieve_docs = retriever.retrieve(solution.source_code, top_k)\n",
    "        reranked_docs = await rerank_docs(problem, solution, retrieve_docs, top_n)\n",
    "        analyses = await describe_examples(reranked_docs)\n",
    "        examplars = format_examples(reranked_docs, analyses)\n",
    "        return examplars\n",
    "\n",
    "    @weave.op\n",
    "    async def rag_solution(\n",
    "            problem: Problem,\n",
    "            draft_solution: Solution,\n",
    "            model: str = STRONG_LLM,\n",
    "            temperature: float = 0.0,\n",
    "            timeout: int = timeout,\n",
    "    ) -> dict:\n",
    "        logger.info(f\"Generating RAG solution:\")\n",
    "        examplars = await create_examplars(problem, draft_solution)\n",
    "        rag_solution = await generate_solution(\n",
    "            problem=problem,\n",
    "            examples=examplars,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        test_report = check_correctness(\n",
    "            rag_solution.source_code,\n",
    "            problem.sample_input,\n",
    "            problem.sample_output,\n",
    "            timeout,\n",
    "        )\n",
    "        logger.info(f\"RAG Solution Result: {repr(test_report)}\")\n",
    "        return {\"solution\": rag_solution, \"test_report\": test_report}\n",
    "\n",
    "    rag_result = await rag_solution(problem, solution, model, temperature, timeout)\n",
    "    solution = rag_result[\"solution\"]\n",
    "    test_report = rag_result[\"test_report\"]\n",
    "    return {\"solution\": solution, \"stage\": \"rag\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result = await rag_solver(retriever, problem, timeout=30)\n",
    "print(\"*\" * 80)\n",
    "print(rag_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(rag_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we are now ready to evaluate the RAG agent.\n",
    "We will create a `weave.Model` instance for the RAG agent and evaluate it using the same evaluation framework we used for the zero-shot agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver(\n",
    "            retriever=self.retriever,\n",
    "            problem=Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG agent for all the models and temperatures\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_agent = RAGAgent(\n",
    "            retriever=retriever, model=LLM, temperature=temperature, timeout=30\n",
    "        )\n",
    "        rag_results = eval.evaluate(rag_agent)\n",
    "        tasks.append(rag_results)\n",
    "\n",
    "# Again, 30 evals for the RAG agent with different models and temperatures\n",
    "\n",
    "rag_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While the RAG agent is an improvement over the zero-shot agent, it's still not perfect.\n",
    "It's still susceptible to hallucinations and incorrect solutions. \n",
    "One way to mitigate this is to use reflection.\n",
    "We can use another LLM call to reflect on the solution and test results and improve it.\n",
    "We can then use the improved solution to generate new few-shot examples and repeat the process in a loop until we converge to a solution or the iteration limit is reached.\n",
    "\n",
    "Again, this is not the best approach to solve the problem and has a lot of room for improvement, but it should help us get towards a working solution.\n",
    "\n",
    "Here are the reflection instructions we will provide to the LLM to reflect on the solution and test results, feel free to change the instructions to improve the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import REFLECTION_INSTRUCTIONS, rework_solution\n",
    "\n",
    "print(REFLECTION_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def rag_solver_with_reflection(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.0,\n",
    "        max_iterations: int = 2,\n",
    "        timeout: int = 10,\n",
    "):\n",
    "    num_iterations = 0\n",
    "    test_report = \"failed\"\n",
    "    solution = None\n",
    "    while not test_report == \"passed\" and num_iterations < max_iterations:\n",
    "        rag_result = await rag_solver(\n",
    "            retriever=retriever,\n",
    "            problem=problem,\n",
    "            timeout=timeout,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        solution = rag_result[\"solution\"]\n",
    "        test_report = rag_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return rag_result\n",
    "        rework_result = await rework_solution(\n",
    "            problem=problem,\n",
    "            incorrect_solution=solution,\n",
    "            test_report=test_report,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        solution = rework_result[\"solution\"]\n",
    "        test_report = rework_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return {\n",
    "                \"solution\": solution,\n",
    "                \"stage\": \"reflection\",\n",
    "                \"test_report\": test_report,\n",
    "            }\n",
    "        num_iterations += 1\n",
    "    logger.info(\"Failed to generate a solution\")\n",
    "    return {\"solution\": solution, \"stage\": \"failed\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_result = await rag_solver_with_reflection(\n",
    "    retriever, problem, max_iterations=2, timeout=30\n",
    ")\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now, we are ready to evaluate a more complex agent that uses reflection\n",
    "This agent will try to solve the problem using the retriever\n",
    "and if it fails, it will ask the model to reflect on the problem\n",
    "and then re-work the solution\n",
    "and repeat this process for a fixed number of iterations\n",
    "or until the solution is correct or the iteration limit is reached\n",
    "\n",
    "But the best part is that we can use the same evaluation framework we used for the zero-shot and RAG agent to evaluate the RAG reflection agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG reflection agent for all the models and temperatures\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_reflection_agent = RAGReflectionAgent(\n",
    "            retriever=retriever, model=LLM, temperature=temperature, timeout=30\n",
    "        )\n",
    "        rag_reflection_results = eval.evaluate(rag_reflection_agent)\n",
    "        tasks.append(rag_reflection_results)\n",
    "rag_reflection_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that completes the demo!\n",
    "\n",
    "Key takeaways from this demo:\n",
    "1. We tried to solve some challenging competitive programming problems using LLM agents.\n",
    "2. We tried three different agents:\n",
    "    - Zero-shot agent\n",
    "    - RAG agent\n",
    "    - RAG reflection agent\n",
    "3. We used Weave to evaluate the agents and compare their performance.\n",
    "\n",
    "We hope you found this demo useful and interesting and that it gave you some ideas on how to use LLM agents to solve challenging problems.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
