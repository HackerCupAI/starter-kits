{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "from nest_asyncio import apply\n",
    "\n",
    "from agent import zero_shot_solver, rag_solver, rag_solver_with_reflection\n",
    "from retriever import Retriever\n",
    "from utils import Problem, FAST_LLM, STRONG_LLM\n",
    "from utils import check_correctness, load_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging to see the progress\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "apply()\n",
    "# Load the environment variables\n",
    "# You might need to set the `OPENAI_API_KEY` and `WANDB_API_KEY` in your environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weave Setup\n",
    "\n",
    "WEAVE_PROJECT = \"parambharat/hackercup\"  # REPLACE WITH YOUR PROJECT NAME\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check to see if the code evaluation works\n",
    "# We will use this to check the programs our the agents generate\n",
    "\n",
    "program_code = \"print('hello, world!')\"\n",
    "input_data = \"\"\n",
    "expected_output = \"hello, world!\"\n",
    "timeout = 2\n",
    "\n",
    "test_result = check_correctness(program_code, input_data, expected_output, timeout)\n",
    "print(\"Example 1: \", test_result)\n",
    "test_result = check_correctness(\"print('goodbye')\", input_data, \"hi there\", timeout)\n",
    "print(\"Example 2: \", test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: DOWNLOAD THE DATASET BEFORE RUNNING THIS\n",
    "# see README.MD for details or \n",
    "# checkout `starter-kits/submit_first_solution/download.py`\n",
    "\n",
    "\n",
    "dataset_dir = Path(\"data/dataset/2023/practice\")\n",
    "problem_names = map(lambda x: x.stem, dataset_dir.rglob(\"*in\"))\n",
    "problems = list(map(lambda x: load_problem(x, dataset_dir), problem_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple evaluation using weave for the Agents\n",
    "# You'll quickly see how this simple evaluation can become very powerful\n",
    "# and will scale to very complex workflows\n",
    "# The agent workflow already takes care of running the code\n",
    "# evaluating the solution against the expected output for the sample test cases\n",
    "# and returning the report in the model output\n",
    "# we expect that the `test_report` is \"passed\" in the agent output\n",
    "# so we can use that to evaluate the agent\n",
    "\n",
    "\n",
    "# This is a simple dataset that contains the problem and the expected output\n",
    "examples = [{\"problem\": problem, \"expected\": \"passed\"} for problem in problems]\n",
    "\n",
    "\n",
    "# This is a simple scorer that checks if the agent output has passed the test\n",
    "@weave.op\n",
    "def scorer(expected: str, model_output: dict) -> dict:\n",
    "    return {'passed': expected == model_output['test_report']}\n",
    "\n",
    "\n",
    "# This is a simple evaluation that checks if the agent output has passed the test\n",
    "eval = weave.Evaluation(\n",
    "    dataset=examples,\n",
    "    scorers=[scorer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple agent that uses zero shot solver\n",
    "# Nothing fancy here, just a model that takes in a problem and returns a solution\n",
    "\n",
    "class ZeroshotAgent(weave.Model):\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await zero_shot_solver(\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run the zero shot agent directly\n",
    "# sample_zeroshot_result = await zero_shot_solver(problems[0], timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the zero shot agent for all the models and temperatures\n",
    "\n",
    "eval_models = [FAST_LLM, STRONG_LLM]\n",
    "eval_temperatures = [0.0, 0.5, 1.0]\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        zeroshot_agent = ZeroshotAgent(model=LLM, temperature=temperature, timeout=30)\n",
    "        zeroshot_results = eval.evaluate(zeroshot_agent)\n",
    "        tasks.append(zeroshot_results)\n",
    "\n",
    "# Phew that's 2(models)*3(temps)*5(problems) = 30 evaluations\n",
    "\n",
    "zeroshot_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the RAG agents, we'll use the same evaluation framework\n",
    "# But first we need to load the retriever\n",
    "# You might need to process the data and create the retriever\n",
    "# checkout `starter-kits/rag/retriever.py` for more details\n",
    "# We will share a `weave.dataset` for the retriever soon and\n",
    "# you can use that to load the retriever instead\n",
    "\n",
    "\n",
    "logger.info(\"Loading retriever ... this may take a while ...\")\n",
    "retriever = Retriever.load(\"data/cache/retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RAG agent is a model that takes in a problem and returns a solution\n",
    "# using the retriever to retrieve the similar problems and the solutions\n",
    "# and then use the model to generate a new solution\n",
    "\n",
    "class RAGAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    model: str = FAST_LLM\n",
    "    temperature: float = 0.0\n",
    "    timeout: int = 30\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver(\n",
    "            retriever=self.retriever,\n",
    "            problem=Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run the RAG agent directly\n",
    "# sample_rag_result = await rag_solver(retriever, problems[0], timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG agent for all the models and temperatures\n",
    "\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_agent = RAGAgent(retriever=retriever, model=LLM, temperature=temperature, timeout=30)\n",
    "        rag_results = eval.evaluate(rag_agent)\n",
    "        tasks.append(rag_results)\n",
    "\n",
    "# Again, 30 evals for the RAG agent with different models and temperatures\n",
    "\n",
    "rag_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, A more complex agent that uses reflection\n",
    "# This agent will try to solve the problem using the retriever\n",
    "# and if it fails, it will ask the model to reflect on the problem\n",
    "# and then re-work the solution\n",
    "# and repeat this process for a fixed number of iterations\n",
    "# or until the solution is correct or the iteration limit is reached\n",
    "\n",
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            timeout=self.timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run the RAG reflection agent directly\n",
    "# sample_rag_reflection_result = await rag_solver_with_reflection(retriever, problems[0], max_iterations=2, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG reflection agent for all the models and temperatures\n",
    "tasks = []\n",
    "for LLM in eval_models:\n",
    "    for temperature in eval_temperatures:\n",
    "        rag_reflection_agent = RAGReflectionAgent(\n",
    "            retriever=retriever, model=LLM,\n",
    "            temperature=temperature, timeout=30)\n",
    "        rag_reflection_results = eval.evaluate(rag_reflection_agent)\n",
    "        tasks.append(rag_reflection_results)\n",
    "rag_reflection_results = await asyncio.gather(*tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
